In order &)*&*#@(&$(*#&$(*@#&$*(&#@*($&#@*(&$*(#&(*$&#*(@&$(*&#(*@&$*(#@&$*(&#@*(&$*(@#&$*(&#@*($&(*#@&$(*&@#*($&*(

Word similarity DONE
Cosine Similarity DONE

Construct the Decision Tree using Information gain and Gini Index DONE
Confusion matrix and compute recall, precision, F1, and so on DONE
The motivation and advantages of random forest DONE
The kernel methods and motivation of SVM method DONE

Activation function motivation + formulas DONE
DNN 30 pts dimensions + motivation of DNN	
2 input perceptron 3 hidden 2 output
input layer (input feauture)
output layer (classification)
y = wx+b
Back prop 3 lines of code DONE
Loss function (know all)
Learning rate motivation DONE
MLP (fully -connected layer) = motivation
input a vector ( flatten into a vector to feed into fc layer)
output size calc
The advantage of Dropout, layer normalization, batch normalization. What is the difference between layer normalization and batch normalization? DONE
gradient descent (coding in tutorial slides 50 pts) X? 
Batch size meaning DONE
layer normalization DONE
early stopping motivation DONE

Convolutional neural networks (code from hw3 layers kernsize stride output and input sizes), max pooling (pooling maxpool2d(calculate outputsize)) DONE

basic torch commands (reshape the tensor, flatten, size of tensor) DONE
07-Pytorch Tutorial (important) except for save checkpoint and tensorboard DONE
highlighted part(train model, back prop) DONE
tensorflow??????? maybe?????? NO NEED

one hot encoding DONE
TF-IDF (term frequency 3 steps) (app adv findings limit practice ex), word2vec, the motivations of LSTM (no math formula yes application also steps 1-4) DONE

data preprocessing for nlp (main idea, no coding but know every step) DONE
tokenize (every step, main idea) DONE
mapping dictionary (padding, unknown) DONE
dictionary of tokens to index DONE
NLP-tutorial except for LSTM architecture DONE

adjacency matrix and google Page rank DONE
Graph embedding? NO NEED

OTHER:
review hw3 (important code)
neural networds API (From hw3 conv layer, cacl output size) DONE
nn.sequential DONE
DEMO CODE DONE
model train and evaluate (with torch.no_grad()) DONE
